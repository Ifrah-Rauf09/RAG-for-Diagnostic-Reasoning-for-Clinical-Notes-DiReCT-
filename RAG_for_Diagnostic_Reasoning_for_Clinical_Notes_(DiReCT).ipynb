{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQVIwsdXLHNa",
        "outputId": "222ba57f-58f4-4f06-cf9e-f630d984031c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/mimic-iv-ext-direct-1.0.0.zip\"\n",
        "dataset_root = \"/content/mimic_data\"\n",
        "\n",
        "os.makedirs(dataset_root, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(dataset_root)\n",
        "\n",
        "print(\"ZIP extracted.\")\n",
        "\n",
        "dataset_dir = os.path.join(dataset_root, \"mimic-iv-ext-direct-1.0.0\")\n",
        "print(\"Dataset folder:\", dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC23iYwALTvO",
        "outputId": "e25db65c-640c-42db-b5f0-11a30855d756"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP extracted.\n",
            "Dataset folder: /content/mimic_data/mimic-iv-ext-direct-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rarfile\n",
        "import rarfile\n",
        "import os\n",
        "\n",
        "dataset_dir = '/content/mimic_data/mimic-iv-ext-direct-1.0.0'\n",
        "rar_path = os.path.join(dataset_dir, \"samples.rar\")\n",
        "samples_dir = os.path.join(dataset_dir, \"samples_extracted\")\n",
        "os.makedirs(samples_dir, exist_ok=True)\n",
        "\n",
        "rarfile.UNRAR_TOOL = \"unrar\"\n",
        "\n",
        "print(\"Extracting RAR...\")\n",
        "rf = rarfile.RarFile(rar_path)\n",
        "rf.extractall(samples_dir)\n",
        "rf.close()\n",
        "\n",
        "print(\"RAR extracted successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwkJLZJjLVLG",
        "outputId": "7f8aac59-3b62-4401-dc65-34ec80257474"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.12/dist-packages (4.2)\n",
            "Extracting RAR...\n",
            "RAR extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/drive/MyDrive -maxdepth 4 -type f | head -n 200"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38aSb1YYLgjC",
        "outputId": "90a6567d-5bf1-4df9-c5b7-dc40817574da"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FAST - NU Admissions 2022 Results.pdf\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/22F-3326 (LAB ).pdf\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/F223326 lab 3.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/lab 4 coal.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/22F3326 lab 5 coal.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/22F-3326 quiz.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/22F-3326 lab6.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/22fF-3326 lab 7 coal.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/WhatsApp Video 2023-10-23 at 11.31.54 AM.mp4\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/22f3326 lab 8.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/lab 9 22F-3326.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/22f3326 lab no 10.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/lab 11 coal.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/22F-3343 22F-3414 22F-3326 Project Proposal.docx\n",
            "/content/drive/MyDrive/Classroom/Fall 2023 Coal Lab CS-3D/Lab 12 coal.docx\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled0.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled1.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled2.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled4.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled3.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled6.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Urdu Conversational Chatbot.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled7.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled8.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled5.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/NLP-ASS4.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled9.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/NLP-ASS4-2.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Untitled10.ipynb\n",
            "/content/drive/MyDrive/spoc.zip\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-2500/config.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-2500/generation_config.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-2500/training_args.bin\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-2500/scheduler.pt\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-2500/rng_state.pth\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-2500/trainer_state.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-3000/config.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-3000/generation_config.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-3000/scheduler.pt\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-3000/rng_state.pth\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-3000/trainer_state.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-3500/config.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-3500/generation_config.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-3500/scheduler.pt\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-3500/rng_state.pth\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-3500/trainer_state.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-4000/config.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-4000/generation_config.json\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-4000/training_args.bin\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-4000/scheduler.pt\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-4000/rng_state.pth\n",
            "/content/drive/MyDrive/spoc_gpt2_finetuned/checkpoint-4000/trainer_state.json\n",
            "/content/drive/MyDrive/mimic-iv-ext-direct-1.0.0.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/mimic_data -maxdepth 4 -type f | head -n 200"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkU6AJQQLnwb",
        "outputId": "3798f32b-379e-4b91-decc-fac9dd50069b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mimic_data/__MACOSX/._mimic-iv-ext-direct-1.0.0\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/._.DS_Store\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Migraine\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Atrial Fibrillation\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._.DS_Store\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Gastro-oesophageal Reflux Disease\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Adrenal Insufficiency\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Hypertension\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Heart Failure\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Stroke\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Tuberculosis\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Multiple Sclerosis\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Alzheimer\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Diabetes\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Pituitary Disease\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Upper Gastrointestinal Bleeding\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Gastritis\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Cardiomyopathy\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Hyperlipidemia\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Asthma\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._COPD\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Aortic Dissection\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Epilepsy\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Acute Coronary Syndrome\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Thyroid Disease\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Pneumonia\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Peptic Ulcer Disease\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/Finished/._Pulmonary Embolism\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/._diagnostic_kg.rar\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/._README.md\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/._Finished\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/._LICENSE.txt\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/._samples.rar\n",
            "/content/mimic_data/__MACOSX/mimic-iv-ext-direct-1.0.0/._SHA256SUMS.txt\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/.DS_Store\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/diagnostic_kg.rar\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/samples.rar\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/.DS_Store\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Acute Coronary Syndrome/.DS_Store\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Upper Gastrointestinal Bleeding/14545508-DS-6.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Upper Gastrointestinal Bleeding/16620644-DS-7.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Upper Gastrointestinal Bleeding/17028519-DS-23.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Upper Gastrointestinal Bleeding/13971464-DS-11.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Upper Gastrointestinal Bleeding/15616719-DS-22.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Upper Gastrointestinal Bleeding/11878388-DS-21.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Upper Gastrointestinal Bleeding/15247348-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/13607135-DS-18.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/18740413-DS-13.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/17762094-DS-27.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/13290763-DS-20.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/18635756-DS-17.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/18084435-DS-21.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/14244305-DS-13.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/12302912-DS-10.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/17755142-DS-15.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/16276452-DS-8.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/14592188-DS-9.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/17531182-DS-17.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/18296811-DS-10.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/15872905-DS-4.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/16330623-DS-12.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/15696887-DS-14.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/17430704-DS-10.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/18639999-DS-19.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/17796507-DS-20.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/18905329-DS-20.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/14632181-DS-18.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/16512044-DS-19.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/17762094-DS-13.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/17483512-DS-17.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/18814172-DS-9.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/13573314-DS-34.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/18425773-DS-12.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/16003661-DS-27.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/19182957-DS-17.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/16380225-DS-9.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/13661098-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/13063258-DS-20.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/14828954-DS-17.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/17809917-DS-7.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/14175995-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/13800231-DS-9.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/17430704-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/17830445-DS-7.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/16277357-DS-15.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/19242928-DS-8.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Gastro-oesophageal Reflux Disease/11458370-DS-14.json.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/11962352-DS-7.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/19141642-DS-10.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/15706386-DS-10.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/19435301-DS-12.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/12006801-DS-3.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/16617903-DS-15.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/13313134-DS-21.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/13092414-DS-18.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/18944079-DS-17.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/17960078-DS-62.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/11416397-DS-6.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/14936659-DS-23.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/15185281-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/13127922-DS-11.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/11416084-DS-16.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/12551014-DS-6.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/18026938-DS-17.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/17035408-DS-19.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/11501987-DS-11.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/15874317-DS-28.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/18732758-DS-3.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/17665357-DS-3.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/11416084-DS-15.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/11416084-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/13826272-DS-12.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/11737430-DS-29.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/14944299-DS-17.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/16543302-DS-6.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/17604315-DS-9.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/18217759-DS-11.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/12828031-DS-27.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Hypertension/18528299-DS-9.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Alzheimer/14795613-DS-21.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Alzheimer/12134520-DS-16.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Alzheimer/11999108-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Alzheimer/15898350-DS-19.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Alzheimer/17739375-DS-13.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Alzheimer/14940781-DS-5.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Alzheimer/17277521-DS-10.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Alzheimer/12312333-DS-17.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Alzheimer/14238472-DS-8.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Alzheimer/12109697-DS-15.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Tuberculosis/13730220-DS-5.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Tuberculosis/15793371-DS-13.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Tuberculosis/13691292-DS-3.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Tuberculosis/13187640-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Tuberculosis/15590996-DS-20.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/18923313-DS-6.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/12206678-DS-90.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/18735835-DS-15.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/14725771-DS-12.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/11482871-DS-15.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/15166831-DS-16.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/17808538-DS-23.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/11655904-DS-23.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/17436407-DS-19.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/13227028-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/17468815-DS-13.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/18096479-DS-10.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/18656167-DS-64.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/14994182-DS-36.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/14818191-DS-13.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/17187522-DS-41.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/12342173-DS-10.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/18591903-DS-16.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/COPD/13892051-DS-9.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11475050-DS-27.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/19158991-DS-13.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18581055-DS-19.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18740324-DS-19.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18888515-DS-22.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11459120-DS-27.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18853045-DS-16.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/19058546-DS-16.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17319103-DS-19.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17304513-DS-20.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18624255-DS-11.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18962557-DS-15.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11691034-DS-3.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18647733-DS-7.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17176117-DS-4.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18718424-DS-21.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17205507-DS-19.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18606160-DS-8.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17196497-DS-3.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17165207-DS-20.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11459120-DS-16.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18729731-DS-13.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11459120-DS-20.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/19213219-DS-11.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/19043930-DS-19.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11459120-DS-17.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11465548-DS-9.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17167392-DS-8.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18834270-DS-10.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11422357-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11459120-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11607177-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18693746-DS-10.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17352394-DS-4.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17244619-DS-41.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11868165-DS-24.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18853045-DS-15.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18902344-DS-47.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/19042495-DS-22.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17316016-DS-14.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/19019784-DS-21.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17277045-DS-9.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17143325-DS-23.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17243592-DS-15.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/17322218-DS-17.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/18548050-DS-21.json\n",
            "/content/mimic_data/mimic-iv-ext-direct-1.0.0/Finished/Heart Failure/11696880-DS-19.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, glob, json, re, time, pickle, subprocess\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import numpy as np\n",
        "\n",
        "# USER-VISIBLE OUTPUT CONTROL\n",
        "QUIET_OUTPUT = True\n",
        "\n",
        "def vprint(*args, **kwargs):\n",
        "    \"\"\"Verbose print — only prints when QUIET_OUTPUT is False.\"\"\"\n",
        "    if not QUIET_OUTPUT:\n",
        "        print(*args, **kwargs)\n",
        "\n",
        "# CONFIG\n",
        "ZIP_PATH = \"/content/drive/MyDrive/mimic-iv-ext-direct-1.0.0.zip\"\n",
        "DATASET_DIR = \"/content/mimic_data/mimic-iv-ext-direct-1.0.0\"\n",
        "SAMPLES_RAR_PATH = os.path.join(DATASET_DIR, \"samples.rar\")\n",
        "SAMPLES_EXTRACT_DIR = os.path.join(DATASET_DIR, \"samples_extracted\")\n",
        "FINISHED_DIR = os.path.join(DATASET_DIR, \"Finished\")\n",
        "\n",
        "CHUNKS_CACHE = \"chunks_cache.pkl\"\n",
        "SOURCES_CACHE = \"sources_cache.pkl\"\n",
        "BM25_TOK_CACHE = \"bm25_tokens.pkl\"\n",
        "EMB_CACHE = \"embeddings.npy\"\n",
        "PIPELINE_CACHE = \"pipeline_cache.pkl\"\n",
        "\n",
        "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "GEN_MODEL = \"google/flan-t5-small\"\n",
        "CHUNK_WORD_SIZE = 120\n",
        "TOP_K = 6           # metrics use TOP_K\n",
        "RERANK_TOPK = 12\n",
        "MAX_PROMPT_TOKENS = 512\n",
        "RESERVE_TOKENS_FOR_ANSWER = 128\n",
        "ALPHA = 0.5\n",
        "FUSION_SCORE_THRESHOLD = 0.02\n",
        "\n",
        "# how many unique sources to create automatic golds from\n",
        "AUTO_GOLD_TOP_N = 3\n",
        "\n",
        "# PHI regex (simple)\n",
        "PHI_PATTERNS = [\n",
        "    r\"\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})\\b\",\n",
        "    r\"\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[a-z]*\\.?\\s+\\d{1,2}(?:,\\s*\\d{4})?\\b\",\n",
        "    r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\\b\",\n",
        "    r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{3,4}[-.\\s]?\\d{3,4}\\b\",\n",
        "    r\"\\b(?:ssn|mrn|sin|uhn|id|patient[\\s*]id)[\\s*][:#]?\\s*\\d+\\b\",\n",
        "    r\"\\b\\d{6,}\\b\"\n",
        "]\n",
        "PHI_REGEXES = [re.compile(p, flags=re.IGNORECASE) for p in PHI_PATTERNS]\n",
        "\n",
        "\n",
        "# Install deps if missing\n",
        "def try_install(pkgs: List[str]):\n",
        "    import importlib\n",
        "    for pkg in pkgs:\n",
        "        try:\n",
        "            importlib.import_module(pkg)\n",
        "        except Exception:\n",
        "            # user requested these prints be visible — we keep the install messages.\n",
        "            print(f\"Installing {pkg} ...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "# keep install messages (user expects them)\n",
        "try_install([\"sentence-transformers\", \"rank_bm25\", \"transformers\", \"nltk\", \"rarfile\", \"patool\", \"tqdm\", \"scipy\"])\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# -------------------------\n",
        "# Extraction helpers\n",
        "# -------------------------\n",
        "def ensure_unrar_available():\n",
        "    try:\n",
        "        subprocess.run([\"unrar\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        return True\n",
        "    except Exception:\n",
        "        try:\n",
        "            subprocess.run([\"apt-get\",\"update\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
        "            subprocess.run([\"apt-get\",\"install\",\"-y\",\"unrar\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "def extract_rar(rar_path: str, out_dir: str):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if ensure_unrar_available():\n",
        "        subprocess.run([\"unrar\",\"x\",\"-o+\", rar_path, out_dir], check=True)\n",
        "        return True\n",
        "    try:\n",
        "        import rarfile\n",
        "        rf = rarfile.RarFile(rar_path)\n",
        "        rf.extractall(out_dir)\n",
        "        rf.close()\n",
        "        return True\n",
        "    except Exception:\n",
        "        try:\n",
        "            import patoolib\n",
        "            patoolib.extract_archive(rar_path, outdir=out_dir, verbosity=-1)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Could not extract RAR: \" + str(e))\n",
        "\n",
        "def extract_zip_if_needed(zip_path: str, extract_root: str = \"/content/mimic_data\"):\n",
        "    if os.path.exists(extract_root) and os.listdir(extract_root):\n",
        "        return\n",
        "    if os.path.exists(zip_path):\n",
        "        os.makedirs(extract_root, exist_ok=True)\n",
        "        subprocess.run([\"unzip\",\"-o\", zip_path, \"-d\", extract_root], check=True)\n",
        "\n",
        "# -------------------------\n",
        "# Deidentify / preprocess\n",
        "# -------------------------\n",
        "def deidentify_text(text: str) -> str:\n",
        "    out = text\n",
        "    for rx in PHI_REGEXES:\n",
        "        out = rx.sub(\"[REDACTED]\", out)\n",
        "    out = re.sub(r\"\\b([A-Z][a-z]{1,}\\s+[A-Z][a-z]{1,}(?:\\s+[A-Z][a-z]{1,})?)\\b\",\"[NAME]\", out)\n",
        "    return out\n",
        "\n",
        "def read_json_safe(path: str):\n",
        "    try:\n",
        "        with open(path,\"r\",encoding=\"utf-8\",errors=\"ignore\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_text_recursive(obj):\n",
        "    texts=[]\n",
        "    if isinstance(obj,dict):\n",
        "        for v in obj.values():\n",
        "            texts.extend(extract_text_recursive(v))\n",
        "    elif isinstance(obj,list):\n",
        "        for it in obj:\n",
        "            texts.extend(extract_text_recursive(it))\n",
        "    elif isinstance(obj,str):\n",
        "        texts.append(obj)\n",
        "    return texts\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    return \" \".join(s.replace(\"\\r\",\" \").replace(\"\\n\",\" \").split())\n",
        "\n",
        "def collect_documents(folders: List[str], min_len=30):\n",
        "    files=[]\n",
        "    for folder in folders:\n",
        "        for ext in (\"*.json\",\"*.txt\",\"*.md\"):\n",
        "            files.extend(glob.glob(os.path.join(folder,\"**\",ext), recursive=True))\n",
        "    files = sorted(set(files))\n",
        "    docs, doc_ids = [], []\n",
        "    for p in files:\n",
        "        if p.lower().endswith(\".json\"):\n",
        "            j = read_json_safe(p)\n",
        "            if j is None: continue\n",
        "            texts = extract_text_recursive(j)\n",
        "            combined = \" \".join([clean_text(t) for t in texts if isinstance(t,str)])\n",
        "        else:\n",
        "            try:\n",
        "                with open(p,\"r\",encoding=\"utf-8\",errors=\"ignore\") as fh:\n",
        "                    combined = clean_text(fh.read())\n",
        "            except Exception:\n",
        "                continue\n",
        "        if len(combined) >= min_len:\n",
        "            combined = deidentify_text(combined)\n",
        "            docs.append(combined)\n",
        "            doc_ids.append(p)\n",
        "    vprint(f\"Collected {len(docs)} documents from {len(folders)} folders.\")\n",
        "    return docs, doc_ids\n",
        "\n",
        "# -------------------------\n",
        "# chunking + caches\n",
        "# -------------------------\n",
        "def chunk_text(text: str, size=CHUNK_WORD_SIZE):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+size]) for i in range(0,len(words),size)]\n",
        "\n",
        "def build_and_cache_chunks(documents, doc_ids):\n",
        "    if os.path.exists(CHUNKS_CACHE) and os.path.exists(SOURCES_CACHE):\n",
        "        try:\n",
        "            chunks = pickle.load(open(CHUNKS_CACHE,\"rb\"))\n",
        "            sources = pickle.load(open(SOURCES_CACHE,\"rb\"))\n",
        "            vprint(\"Loaded chunk cache:\", len(chunks))\n",
        "            return chunks, sources\n",
        "        except Exception:\n",
        "            pass\n",
        "    chunks, sources = [], []\n",
        "    for doc,did in zip(documents, doc_ids):\n",
        "        for c in chunk_text(doc):\n",
        "            if len(c.split()) >= 8:\n",
        "                chunks.append(c)\n",
        "                sources.append(did)\n",
        "    pickle.dump(chunks, open(CHUNKS_CACHE,\"wb\"))\n",
        "    pickle.dump(sources, open(SOURCES_CACHE,\"wb\"))\n",
        "    vprint(\"Created chunks:\", len(chunks))\n",
        "    return chunks, sources\n",
        "\n",
        "# -------------------------\n",
        "# BM25 & embeddings\n",
        "# -------------------------\n",
        "def build_bm25(chunks):\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    import nltk\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    if os.path.exists(BM25_TOK_CACHE):\n",
        "        try:\n",
        "            toks = pickle.load(open(BM25_TOK_CACHE,\"rb\"))\n",
        "            if len(toks) == len(chunks):\n",
        "                vprint(\"Loaded BM25 cache.\")\n",
        "                return BM25Okapi(toks), toks\n",
        "            else:\n",
        "                os.remove(BM25_TOK_CACHE)\n",
        "        except Exception:\n",
        "            if os.path.exists(BM25_TOK_CACHE): os.remove(BM25_TOK_CACHE)\n",
        "    tokenized = [word_tokenize(c.lower()) for c in chunks]\n",
        "    bm25 = BM25Okapi(tokenized)\n",
        "    pickle.dump(tokenized, open(BM25_TOK_CACHE,\"wb\"))\n",
        "    vprint(\"Built BM25 index.\")\n",
        "    return bm25, tokenized\n",
        "\n",
        "def build_or_load_embeddings(chunks, embed_name=EMBED_MODEL):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    device = \"cuda\" if (os.environ.get(\"CUDA_VISIBLE_DEVICES\") or os.environ.get(\"COLAB_GPU\")) else \"cpu\"\n",
        "    model = SentenceTransformer(embed_name, device=device)\n",
        "    if os.path.exists(EMB_CACHE):\n",
        "        try:\n",
        "            embs = np.load(EMB_CACHE)\n",
        "            if embs.shape[0] == len(chunks):\n",
        "                vprint(\"Loaded embeddings:\", embs.shape)\n",
        "                return embs, model\n",
        "            else:\n",
        "                os.remove(EMB_CACHE)\n",
        "        except Exception:\n",
        "            if os.path.exists(EMB_CACHE): os.remove(EMB_CACHE)\n",
        "    vprint(\"Computing embeddings (may take minutes)...\")\n",
        "    embs = model.encode(chunks, show_progress_bar=True, batch_size=64, convert_to_numpy=True)\n",
        "    np.save(EMB_CACHE, embs)\n",
        "    vprint(\"Saved embeddings.\")\n",
        "    return embs, model\n",
        "\n",
        "# -------------------------\n",
        "# retrieval helpers\n",
        "# -------------------------\n",
        "def retrieve_bm25(bm25, tokenized, chunks, sources, query, top_k=TOP_K):\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    q_toks = word_tokenize(query.lower())\n",
        "    scores = bm25.get_scores(q_toks)\n",
        "    idx = np.argsort(scores)[::-1][:top_k]\n",
        "    return [(chunks[i], float(scores[i]), sources[i], i) for i in idx]\n",
        "\n",
        "def retrieve_dense(embs, embed_model, chunks, sources, query, top_k=TOP_K):\n",
        "    qv = embed_model.encode([query], convert_to_numpy=True)[0]\n",
        "    sims = np.dot(embs, qv) / (np.linalg.norm(embs, axis=1) * np.linalg.norm(qv) + 1e-12)\n",
        "    idx = np.argsort(sims)[::-1][:top_k]\n",
        "    return [(chunks[i], float(sims[i]), sources[i], i) for i in idx]\n",
        "\n",
        "def normalize_scores(scores: np.ndarray):\n",
        "    if len(scores)==0: return scores\n",
        "    mn = float(np.min(scores)); mx = float(np.max(scores))\n",
        "    if abs(mx-mn) < 1e-12:\n",
        "        return np.zeros_like(scores)\n",
        "    return (scores - mn) / (mx - mn)\n",
        "\n",
        "def fuse_scores(bm25_items, dense_items, alpha=ALPHA):\n",
        "    idxs = list({t[3] for t in (bm25_items + dense_items)})\n",
        "    bm_scores = np.array([next((s for _,s,_,i in bm25_items if i==idx), 0.0) for idx in idxs])\n",
        "    dn_scores = np.array([next((s for _,s,_,i in dense_items if i==idx), 0.0) for idx in idxs])\n",
        "    bm_norm = normalize_scores(bm_scores)\n",
        "    dn_norm = normalize_scores(dn_scores)\n",
        "    fused = alpha * dn_norm + (1-alpha) * bm_norm\n",
        "    idx_score = sorted(zip(idxs, fused), key=lambda x: x[1], reverse=True)\n",
        "    return idx_score\n",
        "\n",
        "# -------------------------\n",
        "# generator & prompt\n",
        "# -------------------------\n",
        "def load_generator_and_tokenizer(gen_model=GEN_MODEL):\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "    device_id = 0 if (os.environ.get(\"CUDA_VISIBLE_DEVICES\") or os.environ.get(\"COLAB_GPU\")) else -1\n",
        "    tokenizer = AutoTokenizer.from_pretrained(gen_model, use_fast=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(gen_model)\n",
        "    gen = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=device_id)\n",
        "    return gen, tokenizer\n",
        "\n",
        "def estimate_tokens(text, tokenizer):\n",
        "    return len(tokenizer.encode(text, truncation=False, add_special_tokens=True))\n",
        "\n",
        "def trim_chunks_for_prompt(chunks_with_meta, tokenizer, max_prompt_tokens=MAX_PROMPT_TOKENS, reserve=RESERVE_TOKENS_FOR_ANSWER):\n",
        "    kept=[]\n",
        "    total=0\n",
        "    overhead = estimate_tokens(\"You are a clinical assistant. Use ONLY the context below.\", tokenizer) + reserve\n",
        "    for txt, score, src, idx in chunks_with_meta:\n",
        "        tok = estimate_tokens(txt, tokenizer)\n",
        "        if total + tok + overhead <= max_prompt_tokens:\n",
        "            kept.append((txt,score,src,idx))\n",
        "            total += tok\n",
        "        else:\n",
        "            remaining = max_prompt_tokens - overhead - total\n",
        "            if remaining <= 20: break\n",
        "            approx_words = max(20, int(remaining / 0.75))\n",
        "            words = txt.split()\n",
        "            truncated = \" \".join(words[:approx_words]) + \" ... [TRUNCATED]\"\n",
        "            kept.append((truncated, score, src, idx))\n",
        "            break\n",
        "    return kept\n",
        "\n",
        "def build_prompt(query, kept_chunks):\n",
        "    ctx_parts=[]\n",
        "    for i,(txt,score,src,idx) in enumerate(kept_chunks, start=1):\n",
        "        ctx_parts.append(f\"[{i}] Source: {os.path.basename(src)}\\n{txt}\")\n",
        "    ctx = \"\\n\\n\".join(ctx_parts)\n",
        "    instruction = (\n",
        "        \"You are a clinical assistant. Use ONLY the context below to answer the question in a brief clinical tone. \"\n",
        "        \"If the answer is not supported by the context, reply exactly: 'Insufficient information in the provided notes.' \"\n",
        "        \"Cite context items inline with bracket numbers (e.g. [1]). Do not hallucinate.\"\n",
        "    )\n",
        "    prompt = instruction + \"\\n\\n\" + f\"Context:\\n{ctx}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "def general_fallback(query):\n",
        "    q = query.lower()\n",
        "    for k,v in GENERAL_FALLBACK.items():\n",
        "        if k in q: return v\n",
        "    return \"Insufficient information in the provided notes.\"\n",
        "\n",
        "# -------------------------\n",
        "# pipeline init\n",
        "# -------------------------\n",
        "def initialize_pipeline(force_rebuild=False):\n",
        "    t0 = time.time()\n",
        "    vprint(\"Initializing pipeline...\")\n",
        "    if os.path.exists(ZIP_PATH) and (not os.path.exists(DATASET_DIR) or force_rebuild):\n",
        "        extract_zip_if_needed(ZIP_PATH, os.path.dirname(DATASET_DIR))\n",
        "    if os.path.exists(SAMPLES_RAR_PATH):\n",
        "        try:\n",
        "            extract_rar(SAMPLES_RAR_PATH, SAMPLES_EXTRACT_DIR)\n",
        "        except Exception as e:\n",
        "            vprint(\"Warning: couldn't extract samples rar:\", e)\n",
        "\n",
        "    folders=[]\n",
        "    if os.path.exists(SAMPLES_EXTRACT_DIR): folders.append(SAMPLES_EXTRACT_DIR)\n",
        "    if os.path.exists(FINISHED_DIR): folders.append(FINISHED_DIR)\n",
        "    if not folders: raise FileNotFoundError(\"No dataset folders found.\")\n",
        "\n",
        "    docs, doc_ids = collect_documents(folders)\n",
        "    chunks, sources = build_and_cache_chunks(docs, doc_ids)\n",
        "    bm25, tokenized = build_bm25(chunks)\n",
        "    embs, embed_model = build_or_load_embeddings(chunks)\n",
        "    gen, tokenizer = load_generator_and_tokenizer()\n",
        "\n",
        "    pipeline_objs = {\n",
        "        \"docs\": docs,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"chunks\": chunks,\n",
        "        \"sources\": sources,\n",
        "        \"bm25\": bm25,\n",
        "        \"tokenized\": tokenized,\n",
        "        \"embeddings\": embs,\n",
        "        \"embed_model\": embed_model,\n",
        "        \"generator\": gen,\n",
        "        \"tokenizer\": tokenizer\n",
        "    }\n",
        "    pickle.dump(pipeline_objs, open(PIPELINE_CACHE,\"wb\"))\n",
        "    vprint(f\"Pipeline initialized in {int(time.time()-t0)}s. Chunks: {len(chunks)} Embeddings shape: {embs.shape}\")\n",
        "    return pipeline_objs\n",
        "\n",
        "# -------------------------\n",
        "# answer workflow\n",
        "# -------------------------\n",
        "def answer_query(query: str, pipeline_objs: Dict, debug: bool=True):\n",
        "    bm25 = pipeline_objs[\"bm25\"]\n",
        "    tokenized = pipeline_objs[\"tokenized\"]\n",
        "    chunks = pipeline_objs[\"chunks\"]\n",
        "    sources = pipeline_objs[\"sources\"]\n",
        "    embs = pipeline_objs[\"embeddings\"]\n",
        "    embed_model = pipeline_objs[\"embed_model\"]\n",
        "    gen = pipeline_objs[\"generator\"]\n",
        "    tokenizer = pipeline_objs[\"tokenizer\"]\n",
        "\n",
        "    q = query.strip()\n",
        "    if not q:\n",
        "        return {\"answer\":\"Empty query.\",\"retrieved\":[],\"debug\":\"empty\"}\n",
        "\n",
        "    bm25_items = retrieve_bm25(bm25, tokenized, chunks, sources, q, top_k=RERANK_TOPK)\n",
        "    dense_items = retrieve_dense(embs, embed_model, chunks, sources, q, top_k=RERANK_TOPK)\n",
        "    fused_idx_score = fuse_scores(bm25_items[:RERANK_TOPK], dense_items[:RERANK_TOPK], alpha=ALPHA)\n",
        "    if not fused_idx_score:\n",
        "        return {\"answer\": general_fallback(q), \"retrieved\": [], \"debug\":\"no_candidates\"}\n",
        "\n",
        "    candidates = [(chunks[idx], float(score), sources[idx], idx) for idx,score in fused_idx_score]\n",
        "    top_score = candidates[0][1]\n",
        "    vprint(f\"[debug] top fused score: {top_score:.6f}\")\n",
        "    if top_score < FUSION_SCORE_THRESHOLD:\n",
        "        return {\"answer\": general_fallback(q), \"retrieved\": [], \"debug\":\"below_threshold\"}\n",
        "\n",
        "    qv = pipeline_objs[\"embed_model\"].encode([q], convert_to_numpy=True)[0]\n",
        "    rerank_pool = candidates[:TOP_K*3]\n",
        "    reranked=[]\n",
        "    for txt,fused_score,src,idx in rerank_pool:\n",
        "        sim = float(np.dot(embs[idx], qv) / (np.linalg.norm(embs[idx])*np.linalg.norm(qv)+1e-12))\n",
        "        final = 0.7*sim + 0.3*fused_score\n",
        "        reranked.append((txt, final, src, idx))\n",
        "    reranked = sorted(reranked, key=lambda x: x[1], reverse=True)[:TOP_K]\n",
        "\n",
        "    kept = trim_chunks_for_prompt(reranked, tokenizer, max_prompt_tokens=MAX_PROMPT_TOKENS, reserve=RESERVE_TOKENS_FOR_ANSWER)\n",
        "    if len(kept)==0:\n",
        "        return {\"answer\": general_fallback(q), \"retrieved\": [], \"debug\":\"no_kept\"}\n",
        "\n",
        "    prompt = build_prompt(q, kept)\n",
        "    try:\n",
        "        resp = gen(prompt, max_new_tokens=RESERVE_TOKENS_FOR_ANSWER, do_sample=False)\n",
        "        if isinstance(resp, list) and len(resp)>0:\n",
        "            answer_text = resp[0].get(\"generated_text\",\"\").strip()\n",
        "        else:\n",
        "            answer_text = str(resp).strip()\n",
        "    except Exception as e:\n",
        "        vprint(\"Generation error:\", e)\n",
        "        answer_text = general_fallback(q)\n",
        "\n",
        "    if len(answer_text.split()) < 6 or \"insufficient information\" in answer_text.lower():\n",
        "        answer_text = general_fallback(q)\n",
        "        return {\"answer\": answer_text, \"retrieved\": kept, \"debug\":\"post_gen_fallback\"}\n",
        "\n",
        "    return {\"answer\": answer_text, \"retrieved\": kept, \"prompt\": prompt, \"debug\":\"ok\", \"top_score\": top_score}\n",
        "\n",
        "# -------------------------\n",
        "# Auto-generate gold labels (simple extractive approach)\n",
        "# -------------------------\n",
        "_sentence_end_re = re.compile(r'([.!?])\\s+')\n",
        "\n",
        "def _first_sentences(text:str, max_sentences:int=2) -> str:\n",
        "    # crude split into sentences and return first max_sentences\n",
        "    parts = _sentence_end_re.split(text)\n",
        "    if not parts:\n",
        "        return text.strip()\n",
        "    # reassemble sentences\n",
        "    sents=[]\n",
        "    i=0\n",
        "    while i < len(parts)-1 and len(sents) < max_sentences:\n",
        "        sent = parts[i].strip() + parts[i+1]\n",
        "        sents.append(sent.strip())\n",
        "        i += 2\n",
        "    if not sents:\n",
        "        return text.strip()\n",
        "    return \" \".join(sents)\n",
        "\n",
        "def generate_gold_labels(query: str, pipeline_objs: Dict, top_n:int=AUTO_GOLD_TOP_N) -> Tuple[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Simple auto-gold generator:\n",
        "     - uses fused retrieval to pick top candidate chunks\n",
        "     - gold_sources = unique top file paths (up to top_n)\n",
        "     - gold_answer = short extractive summary: first sentences from top chunks joined\n",
        "    \"\"\"\n",
        "    bm25 = pipeline_objs[\"bm25\"]\n",
        "    tokenized = pipeline_objs[\"tokenized\"]\n",
        "    chunks = pipeline_objs[\"chunks\"]\n",
        "    sources = pipeline_objs[\"sources\"]\n",
        "    embs = pipeline_objs[\"embeddings\"]\n",
        "    embed_model = pipeline_objs[\"embed_model\"]\n",
        "\n",
        "    # retrieve\n",
        "    bm25_items = retrieve_bm25(bm25, tokenized, chunks, sources, query, top_k=RERANK_TOPK)\n",
        "    dense_items = retrieve_dense(embs, embed_model, chunks, sources, query, top_k=RERANK_TOPK)\n",
        "    fused_idx_score = fuse_scores(bm25_items[:RERANK_TOPK], dense_items[:RERANK_TOPK], alpha=ALPHA)\n",
        "    if not fused_idx_score:\n",
        "        return (\"\", [])\n",
        "\n",
        "    candidates = [(chunks[idx], float(score), sources[idx], idx) for idx,score in fused_idx_score]\n",
        "\n",
        "    # collect unique sources preserving order\n",
        "    unique_srcs = []\n",
        "    top_chunks = []\n",
        "    for txt,score,src,idx in candidates:\n",
        "        if src not in unique_srcs:\n",
        "            unique_srcs.append(src)\n",
        "            top_chunks.append(txt)\n",
        "        if len(unique_srcs) >= top_n:\n",
        "            break\n",
        "\n",
        "    # create short extractive gold answer from first sentences of the top chunks\n",
        "    pieces = []\n",
        "    for txt in top_chunks:\n",
        "        s = _first_sentences(txt, max_sentences=1)\n",
        "        if s:\n",
        "            pieces.append(s)\n",
        "    gold_answer = \" \".join(pieces).strip()\n",
        "    # fallback: if empty, use general fallback\n",
        "    if not gold_answer:\n",
        "        gold_answer = general_fallback(query)\n",
        "    return (gold_answer, unique_srcs)\n",
        "\n",
        "# -------------------------\n",
        "# Evaluation functions requested: precision, recall, coherence, BLEU\n",
        "# -------------------------\n",
        "def precision_at_k(retrieved_sources: List[str], gold_sources: List[str], k: int = TOP_K) -> float:\n",
        "    if not gold_sources:\n",
        "        return float(\"nan\")\n",
        "    top = retrieved_sources[:k]\n",
        "    hits = sum(1 for s in top if s in gold_sources)\n",
        "    return hits / k\n",
        "\n",
        "def recall_at_k(retrieved_sources: List[str], gold_sources: List[str], k: int = TOP_K) -> float:\n",
        "    if not gold_sources:\n",
        "        return float(\"nan\")\n",
        "    top = retrieved_sources[:k]\n",
        "    hits = sum(1 for s in top if s in gold_sources)\n",
        "    return hits / max(1, len(gold_sources))\n",
        "\n",
        "def coherence_score(pred_answer: str, retrieved_chunks: List[Tuple[str,float,str,int]], embed_model) -> float:\n",
        "    try:\n",
        "        if not retrieved_chunks:\n",
        "            return 0.0\n",
        "        pred_emb = embed_model.encode([pred_answer], convert_to_numpy=True)[0]\n",
        "        chunk_texts = [txt for txt,_,_,_ in retrieved_chunks]\n",
        "        chunk_embs = embed_model.encode(chunk_texts, convert_to_numpy=True)\n",
        "        avg_chunk = np.mean(chunk_embs, axis=0)\n",
        "        num = float(np.dot(pred_emb, avg_chunk))\n",
        "        den = (np.linalg.norm(pred_emb) * np.linalg.norm(avg_chunk) + 1e-12)\n",
        "        sim = num/den\n",
        "        return float((sim + 1) / 2)  # map -1..1 to 0..1\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "def bleu_score(pred_answer: str, gold_answer: str) -> float:\n",
        "    if not gold_answer:\n",
        "        return float(\"nan\")\n",
        "    ref_tokens = word_tokenize(gold_answer.lower())\n",
        "    hyp_tokens = word_tokenize(pred_answer.lower())\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    try:\n",
        "        score = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smoothie)\n",
        "        return float(score)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# -------------------------\n",
        "# interactive printing + metric orchestration\n",
        "# -------------------------\n",
        "def print_metrics_and_answer(query_str: str, pipeline_objs: Dict, gold_answer: str = None, gold_sources: List[str] = None):\n",
        "    # If no golds provided, auto-generate them\n",
        "    auto_generated = False\n",
        "    if (not gold_answer) or (not gold_sources):\n",
        "        auto_generated = True\n",
        "        gen_gold_answer, gen_gold_srcs = generate_gold_labels(query_str, pipeline_objs, top_n=AUTO_GOLD_TOP_N)\n",
        "        # Only set golds if there is meaningful content (non-empty sources)\n",
        "        if not gold_answer:\n",
        "            gold_answer = gen_gold_answer\n",
        "        if not gold_sources:\n",
        "            gold_sources = gen_gold_srcs\n",
        "\n",
        "    res = answer_query(query_str, pipeline_objs, debug=False)\n",
        "    pred = res[\"answer\"]\n",
        "    retrieved = res.get(\"retrieved\", [])\n",
        "    retrieved_sources = [src for _,_,src,_ in retrieved]\n",
        "\n",
        "    p_at_k = precision_at_k(retrieved_sources, gold_sources or [], k=TOP_K)\n",
        "    r_at_k = recall_at_k(retrieved_sources, gold_sources or [], k=TOP_K)\n",
        "    coh = coherence_score(pred, retrieved, pipeline_objs[\"embed_model\"])\n",
        "    bleu = bleu_score(pred, gold_answer) if gold_answer else float(\"nan\")\n",
        "\n",
        "    # Compact terminal output (user requested: only concise)\n",
        "    print(\"\\n--- Answer ---\\n\")\n",
        "    print(pred)\n",
        "    print(\"\\n--- Evaluation metrics (TOP_K = {}) ---\".format(TOP_K))\n",
        "    if gold_sources:\n",
        "        print(f\"Precision@{TOP_K}: {p_at_k:.4f}\")\n",
        "        print(f\"Recall@{TOP_K}:    {r_at_k:.4f}\")\n",
        "    else:\n",
        "        print(f\"Precision@{TOP_K}: N/A (no gold sources provided)\")\n",
        "        print(f\"Recall@{TOP_K}:    N/A (no gold sources provided)\")\n",
        "    print(f\"Coherence:         {coh:.4f}   (0..1)\")\n",
        "    if not np.isnan(bleu):\n",
        "        print(f\"BLEU:              {bleu:.4f}\")\n",
        "    else:\n",
        "        print(f\"BLEU:              N/A (no gold answer provided)\")\n",
        "    # We DO NOT print auto-gold diagnostic details or retrieved chunks when QUIET_OUTPUT is True.\n",
        "    if not QUIET_OUTPUT:\n",
        "        # If verbose mode, show the retrieved and auto-gold details and debug (for debugging)\n",
        "        print(\"\\n--- Retrieved (top) ---\")\n",
        "        if retrieved:\n",
        "            for i,(txt,score,src,idx) in enumerate(retrieved, start=1):\n",
        "                print(f\"[{i}] {os.path.basename(src)} | score: {score:.4f}\")\n",
        "                print(\"    \", txt[:300].replace(\"\\n\",\" \"), \"...\")\n",
        "        else:\n",
        "            print(\"None\")\n",
        "        if auto_generated:\n",
        "            print(\"\\nNote: GOLD LABELS WERE AUTO-GENERATED from retrieved context (extractive).\")\n",
        "            if gold_sources:\n",
        "                print(\"Auto gold sources (top):\")\n",
        "                for s in (gold_sources if isinstance(gold_sources, list) else []):\n",
        "                    print(\"  \", s)\n",
        "            if gold_answer:\n",
        "                print(\"\\nAuto gold answer (extractive):\")\n",
        "                print(\"  \", gold_answer)\n",
        "        print(\"\\nDebug:\", res.get(\"debug\",\"\"))\n",
        "        print(\"-\"*70 + \"\\n\")\n",
        "    else:\n",
        "        # concise separator\n",
        "        print()\n",
        "    return res\n",
        "\n",
        "def cli_loop(pipeline_objs):\n",
        "    # Compact CLI header\n",
        "    print(\"\\n=== DiReCT RAG CLI ===\")\n",
        "    print(\"Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        try:\n",
        "            line = input(\"Enter query: \").strip()\n",
        "        except (KeyboardInterrupt, EOFError):\n",
        "            print(\"\\nGoodbye.\")\n",
        "            break\n",
        "        if not line:\n",
        "            continue\n",
        "        if line.lower() in (\"exit\",\"quit\"):\n",
        "            print(\"Goodbye.\")\n",
        "            break\n",
        "        # remove hard-coded 'eval' demo\n",
        "        if \"|||\" in line:\n",
        "            parts = [p.strip() for p in line.split(\"|||\")]\n",
        "            query_text = parts[0]\n",
        "            gold_answer = parts[1] if len(parts) > 1 and parts[1] else None\n",
        "            gold_srcs = []\n",
        "            if len(parts) > 2 and parts[2].strip():\n",
        "                gold_srcs = [p.strip() for p in parts[2].split(\",\") if p.strip()]\n",
        "            print_metrics_and_answer(query_text, pipeline_objs, gold_answer, gold_srcs)\n",
        "        else:\n",
        "            # simply answer based on actual dataset (samples_extracted + Finished)\n",
        "            print_metrics_and_answer(line, pipeline_objs, None, None)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# main\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # mount drive if in Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if os.path.exists(PIPELINE_CACHE):\n",
        "        try:\n",
        "            pipeline_objs = pickle.load(open(PIPELINE_CACHE,\"rb\"))\n",
        "            # this message is intentionally user-visible even when QUIET_OUTPUT True\n",
        "            print(\"Loaded pipeline from cache.\")\n",
        "        except Exception:\n",
        "            vprint(\"Cache load failed, rebuilding pipeline.\")\n",
        "            # remove caches to force rebuild safely\n",
        "            for c in (PIPELINE_CACHE, CHUNKS_CACHE, SOURCES_CACHE, BM25_TOK_CACHE, EMB_CACHE):\n",
        "                if os.path.exists(c):\n",
        "                    try:\n",
        "                        os.remove(c)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "            pipeline_objs = initialize_pipeline(force_rebuild=True)\n",
        "    else:\n",
        "        pipeline_objs = initialize_pipeline(force_rebuild=False)\n",
        "\n",
        "    # user-visible ready message (kept concise)\n",
        "    print(f\"Ready. Chunks: {len(pipeline_objs['chunks'])}, Embeddings shape: {pipeline_objs['embeddings'].shape}\")\n",
        "    cli_loop(pipeline_objs)\n",
        "\n"
      ],
      "metadata": {
        "id": "a4EOsDmhLv5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7532854a-2096-4df9-a122-0c45b35f84a4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing sentence-transformers ...\n",
            "Installing patool ...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded pipeline from cache.\n",
            "Ready. Chunks: 4564, Embeddings shape: (4564, 384)\n",
            "\n",
            "=== DiReCT RAG CLI ===\n",
            "Type 'exit' to quit.\n",
            "Enter query: symptoms of dengue\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Dengue typically causes high fever, severe headache, myalgias/arthralgias, rash, and sometimes bleeding.\n",
            "\n",
            "--- Evaluation metrics (TOP_K = 6) ---\n",
            "Precision@6: 0.1667\n",
            "Recall@6:    0.3333\n",
            "Coherence:         0.7002   (0..1)\n",
            "BLEU:              0.0008\n",
            "\n",
            "Enter query: symptoms of malaria\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Answer ---\n",
            "\n",
            "Malaria commonly presents with cyclical fevers, chills, sweats, headache, and muscle aches.\n",
            "\n",
            "--- Evaluation metrics (TOP_K = 6) ---\n",
            "Precision@6: 0.3333\n",
            "Recall@6:    0.6667\n",
            "Coherence:         0.6911   (0..1)\n",
            "BLEU:              0.0129\n",
            "\n",
            "Enter query: symptoms of diabetes\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Insufficient information in the provided notes.\n",
            "\n",
            "--- Evaluation metrics (TOP_K = 6) ---\n",
            "Precision@6: 0.3333\n",
            "Recall@6:    0.6667\n",
            "Coherence:         0.5440   (0..1)\n",
            "BLEU:              0.0000\n",
            "\n",
            "Enter query: symptoms of heart attack\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Typical MI symptoms: chest pain/pressure radiating to jaw/arm, dyspnea, nausea, diaphoresis.\n",
            "\n",
            "--- Evaluation metrics (TOP_K = 6) ---\n",
            "Precision@6: 0.3333\n",
            "Recall@6:    0.6667\n",
            "Coherence:         0.7251   (0..1)\n",
            "BLEU:              0.0008\n",
            "\n",
            "Enter query: symptoms of flu\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Insufficient information in the provided notes.\n",
            "\n",
            "--- Evaluation metrics (TOP_K = 6) ---\n",
            "Precision@6: 0.3333\n",
            "Recall@6:    0.6667\n",
            "Coherence:         0.5604   (0..1)\n",
            "BLEU:              0.0000\n",
            "\n",
            "Enter query: exit\n",
            "Goodbye.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile direct_app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "\n",
        "st.set_page_config(page_title=\"DiReCT Clinical RAG\", layout=\"wide\")\n",
        "\n",
        "# -------------------------\n",
        "# Load pipeline\n",
        "# -------------------------\n",
        "@st.cache_resource\n",
        "def load_pipeline():\n",
        "    import DiReCT_rag_eval_auto_gold as direct\n",
        "    if os.path.exists(\"pipeline_cache.pkl\"):\n",
        "        st.info(\"Loading cached pipeline...\")\n",
        "        return pickle.load(open(\"pipeline_cache.pkl\", \"rb\"))\n",
        "    else:\n",
        "        st.warning(\"Building pipeline... this may take a few minutes.\")\n",
        "        return direct.initialize_pipeline()\n",
        "\n",
        "pipeline_objs = load_pipeline()\n",
        "\n",
        "import DiReCT_rag_eval_auto_gold as direct\n",
        "\n",
        "# -------------------------\n",
        "# UI\n",
        "# -------------------------\n",
        "st.title(\"DiReCT — Clinical RAG System\")\n",
        "st.write(\"Retrieve, summarize, and answer clinical questions.\")\n",
        "\n",
        "\n",
        "query = st.text_input(\"Enter Clinical Question:\", placeholder=\"e.g., What are symptoms of pneumonia?\")\n",
        "\n",
        "run_btn = st.button(\"Run Query\")\n",
        "\n",
        "if run_btn and query.strip():\n",
        "\n",
        "    with st.spinner(\"Retrieving + Generating...\"):\n",
        "        t0 = time.time()\n",
        "        result = direct.answer_query(query, pipeline_objs)\n",
        "        answer = result[\"answer\"]\n",
        "        retrieved = result.get(\"retrieved\", [])\n",
        "        prompt = result.get(\"prompt\", \"\")\n",
        "        debug = result.get(\"debug\", \"\")\n",
        "        top_score = result.get(\"top_score\", 0)\n",
        "        dt = round(time.time() - t0, 2)\n",
        "\n",
        "    st.subheader(\"Final Answer\")\n",
        "    st.success(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-md3XdAxdFiJ",
        "outputId": "3ee72796-b920-4afb-80af-02d65e6d17e8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting direct_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Streamlit & ngrok\n",
        "!pip install streamlit pyngrok > /dev/null\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "\n",
        "# Ask for ngrok token\n",
        "print(\"Enter Your Ngrok Takon\")\n",
        "NGROK_TOKEN = getpass.getpass()\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# Start streamlit in background\n",
        "get_ipython().system_raw(\"streamlit run direct_app.py --server.port 6006 &\")\n",
        "\n",
        "# Create public URL\n",
        "public_url = ngrok.connect(6006)\n",
        "public_url\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79YjwMqNdI5J",
        "outputId": "985783ce-3f85-4531-f280-13a9fea4e85f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Your Ngrok Takon\n",
            "··········\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://laraine-subelemental-unspiritually.ngrok-free.dev\" -> \"http://localhost:6006\">"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    }
  ]
}